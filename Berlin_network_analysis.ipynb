{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import powerlaw as pl\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=RuntimeWarning)\n",
    "\n",
    "plt.rcParams['figure.figsize']=[15,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.read_edgelist(\"complete_networld_cap.edgelist\", create_using = nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(g):\n",
    "    edge_count = {}\n",
    "    for i in range(len(g.nodes)):\n",
    "        start = str(np.random.randint(1,12982))\n",
    "        status = True\n",
    "        while status == True:\n",
    "            for j in range(1,9):\n",
    "                node_neighbors = list(g.neighbors(start))\n",
    "                if len(node_neighbors) != 0:\n",
    "                    N = random.choice(list(g.neighbors(start)))\n",
    "                    pair = start + ' ' + N\n",
    "\n",
    "                    try: \n",
    "                        edge_count[pair] += 1\n",
    "                    except KeyError as e:\n",
    "                        edge_count[pair] = 1   \n",
    "                    start = N\n",
    "\n",
    "                else:\n",
    "                    status = False\n",
    "            status = False \n",
    "    return edge_count\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotnormal(g):\n",
    "    walk = random_walk(g)\n",
    "    degreesdistribution = dict(Counter(walk.values()))\n",
    "#    print(degreesdistribution)\n",
    "    totaledges = sum(degreesdistribution.values())\n",
    "    length = len(degreesdistribution)\n",
    "    new = {}\n",
    "    for key in degreesdistribution:\n",
    "        new[key] = degreesdistribution[key]/key\n",
    "    \n",
    "#        new[key] = degreesdistribution[key]/totaledges\n",
    "#        degreesdistribution[key] = 1/degreesdistribution[key]\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.scatter(new.keys(),new.values())\n",
    "    plt.title(\"Normalized\")\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.scatter(degreesdistribution.keys(),degreesdistribution.values())\n",
    "    plt.title(\"Not Normalized\")\n",
    "    plt.show()\n",
    "plotnormal(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_list(alist):\n",
    "    avg_list = [sum(elem)/len(elem) for elem in zip(*alist)]\n",
    "    \n",
    "    return avg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resilience_testing(g, runs):\n",
    "    \n",
    "    avg_weak_comp_list, avg_strong_comp_list, avg_total_comp_list, avg_edge_percentage_value_list = [], [], [], []\n",
    "    \n",
    "    for run in range(runs):\n",
    "        print(\"Run: \", run+1)\n",
    "    \n",
    "        g = nx.read_edgelist(\"complete_networld.edgelist\", create_using = nx.DiGraph())\n",
    "        edge_count = random_walk(g)\n",
    "        ordered_edges = sorted(edge_count, key=edge_count.get, reverse=True)\n",
    "        \n",
    "        comp_count, weakly_comp_count, strongly_comp_count, edge_percentage = [], [], [], []\n",
    "        weakly_comp_count.append(nx.number_weakly_connected_components(g))\n",
    "        strongly_comp_count.append(nx.number_strongly_connected_components(g))\n",
    "        comp_count.append(weakly_comp_count[0] + strongly_comp_count[0])\n",
    "        edge_percentage.append(0/len(g.edges())*100)\n",
    "        total_edge_count = len(g.edges())\n",
    "\n",
    "        for i, elem in enumerate(ordered_edges):\n",
    "            elem = elem.split(' ')\n",
    "            g.remove_edge(elem[0],elem[1])\n",
    "\n",
    "            if i % 100 == 0:\n",
    "\n",
    "                weakly_comp = nx.number_weakly_connected_components(g)\n",
    "                strongly_comp = nx.number_strongly_connected_components(g)\n",
    "\n",
    "                comp_count.append(weakly_comp + strongly_comp)\n",
    "                weakly_comp_count.append(weakly_comp)\n",
    "                strongly_comp_count.append(strongly_comp)\n",
    "\n",
    "                edge_percentage_value = i/total_edge_count * 100\n",
    "                edge_percentage.append(edge_percentage_value)\n",
    "                \n",
    "        avg_weak_comp_list.append(weakly_comp_count)\n",
    "        avg_strong_comp_list.append(strongly_comp_count)\n",
    "        avg_total_comp_list.append(comp_count)\n",
    "        avg_edge_percentage_value_list.append(edge_percentage)\n",
    "    \n",
    "    final_weak = avg_list(avg_weak_comp_list)\n",
    "    final_strong =  avg_list(avg_strong_comp_list)\n",
    "    final_total = avg_list(avg_total_comp_list)\n",
    "    final_percentage = avg_list(avg_edge_percentage_value_list)\n",
    "\n",
    "    plt.plot(final_percentage, final_weak, label=\"weak\")\n",
    "    plt.plot(final_percentage, final_strong, label=\"strong\")\n",
    "    plt.plot(final_percentage, final_total, label=\"total\")\n",
    "    plt.xlabel('Edge Removal Percentage')\n",
    "    plt.ylabel('Number of Components')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resilience_testing(g, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_info(g):\n",
    "    print(\"# of nodes: %d\" % g.number_of_nodes())\n",
    "    print(\"# of edges: %d\" % g.number_of_edges())\n",
    "    print(\"# of selfloops: %d\" % g.number_of_selfloops())\n",
    "    print(\"Cycle in g:\")\n",
    "    print(nx.find_cycle(g))\n",
    "    print(\"G is a DAG: %s\" % nx.is_directed_acyclic_graph(g))\n",
    "    print(\"G is a tree: %s\" % nx.is_tree(g))\n",
    "    \n",
    "    print(\"G's density: %1.2f%%\" % (100 * nx.density(g))) #density = number_of_edges / max_theoretical_number_of_edges\n",
    "    print(\"G's reciprocity: %1.2f%%\" % (100 * nx.reciprocity(g))) #Reciprocity: share of connections going both ways in directed graph\n",
    "    print(\"G's clustering coefficient = %s\" % nx.transitivity(g))\n",
    "    print(\"G is a strongly connected graph: %s\" % nx.is_strongly_connected(g))\n",
    "    print(\"G is a weakly connected graph: %s\" % nx.is_weakly_connected(g))\n",
    "    \n",
    "    max_weakly_cc = max(nx.weakly_connected_components(g), key=len)\n",
    "    min_weakly_cc = min(nx.weakly_connected_components(g), key=len)\n",
    "    number_weakly_cc = [len(c) for c in sorted(nx.weakly_connected_components(g), key=len, reverse=True)]\n",
    "    \n",
    "    print(\"Biggest weakly connected component: %s\" % max_weakly_cc)\n",
    "    print(\"Smallest weakly connected component: %s\" % min_weakly_cc)\n",
    "    print(\"# of nodes in the connected components in {0}: {1}\".format(\"G\",number_weakly_cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_analysis(g):\n",
    "    degree = nx.degree(g)\n",
    "    degree_count = Counter(dict(degree).values())\n",
    "    degree_df = pd.DataFrame(list(degree_count.items()), columns = ('k', 'count'))\n",
    "    degree_df = degree_df.sort_values(by='k')\n",
    "    degree_df['p(k)'] = degree_df['count']/degree_df['count'].sum()\n",
    "    degree_df['log_k']= np.log10(degree_df['k'])\n",
    "    degree_df['log_p(k)']= np.log10(degree_df['p(k)'])\n",
    "    print(degree_df.head())\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Linear Scale')\n",
    "    plt.scatter(degree_df['k'], degree_df['p(k)'])\n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Logarithmic Scale')\n",
    "    plt.scatter(degree_df['log_k'], degree_df['log_p(k)']) #Have a thoughtfull look at the x labels (these are powers [2 means 10^2, 3 means 10^3])\n",
    "    #another way to plot in the log_log scale without calculating the logs is to do the following\n",
    "    #plt.loglog(degree_df['k'], degree_df['p(k)'])\n",
    "    plt.show()\n",
    "    \n",
    "    degree_df = degree_df.sort_values(by='k', ascending=False)\n",
    "    degree_df['comsum_p(k)'] = degree_df['p(k)'].cumsum()\n",
    "    degree_df['log_comsum_p(k)'] = np.log10(degree_df['comsum_p(k)'])\n",
    "    print(degree_df.head())\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Linear Scale\")\n",
    "    plt.scatter(degree_df['k'], degree_df['comsum_p(k)'])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Logarithmic Scale\")\n",
    "    plt.scatter(degree_df['log_k'],degree_df['log_comsum_p(k)'])\n",
    "    #try also\n",
    "    #plt.loglog(degree_df['k'], degree_df['comsum_p(k)'])\n",
    "    plt.show()\n",
    "    \n",
    "    slope,intercept,r_value,p_value,std_error = linregress(np.log10(degree_df['k']),np.log10(degree_df['comsum_p(k)']))\n",
    "    print(r_value,p_value)\n",
    "    \n",
    "    #Let us plot both our distribtuion and the straight line returned by the linear regression\n",
    "    plt.clf()\n",
    "    plt.scatter(degree_df['log_k'],degree_df['log_comsum_p(k)'], label=\"degree distribution\")\n",
    "\n",
    "    plt.plot(degree_df['log_k'],intercept+slope*degree_df['log_k'], 'r--', label=\"linear regression fit\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlaw(g):\n",
    "\n",
    "    degrees= sorted(dict(g.degree()).values())\n",
    "\n",
    "    results = pl.Fit(degrees, discrete=True)\n",
    "\n",
    "    #First check whether power_law or expoenential distribution is preferred, then whether power_law or lognormal is preferred\n",
    "    R, p = results.distribution_compare('power_law', 'exponential') #Here we are checking which of those two functions can be a better fit to our distribtuin ('exponential' vs 'power law')\n",
    "    print(R, p)\n",
    "    if (R > 0) and (p < .05): #[This means that the power law assumption is preffered over the exponential, but still we have to do another test to have our final claim]\n",
    "        R, p = results.distribution_compare('power_law', 'lognormal') #Here we are checking which of those two functions can be a better fit to our distribtuin ('lognormal' vs 'power law')\n",
    "        print(R, p)\n",
    "        if p < .05: #This means that result we get is significant (statistically) and we can trust it.\n",
    "            if R > 0:\n",
    "                print(\"Powerlaw hypothesis preferred (p = %1.4f, CDF exponent = %1.4f)\" % (p, results.power_law.alpha - 1)) # The CDF's alpha is the PMF's alpha minus one. The powerlaw package gives us the PMF's alpha.\n",
    "            else:\n",
    "                print(\"Lognormal hypothesis preferred (p = %1.4f, mu = %1.4f, sigma = %1.4f)\" % (p, results.lognormal.mu, results.lognormal.sigma))\n",
    "        else: #We can not trust the result, and we are not able to differenciate even if R > 0 \n",
    "            print(\"Powerlaw and Lognormal are indistinguishable hypotesis.\")\n",
    "    else:\n",
    "        print(\"We cannot rule out an exponential fit. Definitely not a power law.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree(g):\n",
    "    degree_sequence = sorted([d for n, d in G.degree()], reverse=True)  # degree sequence\n",
    "    # print \"Degree sequence\", degree_sequence\n",
    "    degreeCount = collections.Counter(degree_sequence)\n",
    "    deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(deg, cnt, width=0.80, color='b')\n",
    "\n",
    "    plt.title(\"Degree Histogram\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Degree\")\n",
    "    ax.set_xticks([d + 0.4 for d in deg])\n",
    "    ax.set_xticklabels(deg)\n",
    "\n",
    "    # draw graph in inset\n",
    "    plt.axes([0.4, 0.4, 0.5, 0.5])\n",
    "    Gcc = sorted(nx.weakly_connected_component_subgraphs(g), key=len, reverse=True)[0]\n",
    "    pos = nx.spring_layout(g)\n",
    "    plt.axis('off')\n",
    "    nx.draw_networkx_nodes(g, pos, node_size=20)\n",
    "    nx.draw_networkx_edges(g, pos, alpha=0.4)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centrality_measures(g):\n",
    "    deg_cen = nx.degree_centrality(g)\n",
    "    bet_cen = nx.betweenness_centrality(g)\n",
    "    close_cen = nx.closeness_centrality(g)\n",
    "    pagerank_cen = nx.pagerank(g)\n",
    "    \n",
    "    pos = nx.spring_layout(g,iterations=100)\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.title(\"Degree\")\n",
    "    nx.draw_networkx(G,\n",
    "                pos=pos,\n",
    "                node_size=[size*1000 for size in deg_cen.values()],\n",
    "                with_labels = False,\n",
    "                width=0.1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.title(\"Between Centrality\")\n",
    "    nx.draw_networkx(g,\n",
    "                pos=pos,\n",
    "                node_size=[size*1000 for size in bet_cen.values()],\n",
    "                with_labels = False,\n",
    "                width=0.1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.title(\"Closeness Centrality\")\n",
    "    nx.draw_networkx(g,\n",
    "                pos=pos,\n",
    "                node_size=[size*1000 for size in close_cen.values()],\n",
    "                with_labels = False,\n",
    "                width=0.1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.title(\"Page Rank\")\n",
    "    nx.draw_networkx(g,\n",
    "                pos=pos,\n",
    "                node_size=[size*1000 for size in pagerank_cen.values()],\n",
    "                with_labels = False,\n",
    "                width=0.1)\n",
    "\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_cores(g):\n",
    "    pos = nx.spring_layout(g,iterations=100)\n",
    "    deg_cen = nx.degree_centrality(g)\n",
    "    nodes_cores = nx.core_number(g)\n",
    "\n",
    "    nx.draw_networkx(g,\n",
    "                pos=pos,\n",
    "                node_color = 'red',\n",
    "                node_size = [size*1000 for size in deg_cen.values()],\n",
    "                with_labels=True,\n",
    "                width=0.1,\n",
    "                labels=nodes_cores)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_shortests_path_hist(g):\n",
    "    avg_path_length = []\n",
    "    for n in g.nodes():\n",
    "        shortes_paths_lengths = nx.single_source_shortest_path_length(g,n).values()\n",
    "        #print(shortes_paths_lengths)\n",
    "        avg_path_length.append(sum(shortes_paths_lengths)/(len(shortes_paths_lengths)))\n",
    "    plt.clf()\n",
    "    plt.hist(avg_path_length) #avg degree of separation\n",
    "    plt.axvline(nx.average_shortest_path_length(g), color='red', linestyle='dashed')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berlin Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_analysis(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "powerlaw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_measures(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_cores(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shortests_path_hist(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
